{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de Paralelización y Tips Rendimiento Big Data\n",
    "En el primer ejemplo revisamos como lanzar 4 tareas(threads) en paralelo\n",
    "\n",
    "**Nota**: Jupyter no es óptimo para correr tareas en paralelo, en particular la versión de Windows de Ipython no extrae el máximo provecho de las librerias de threading, se recomienda usar python desde la consola para grandes volumenes de datos/procesamiento\n",
    "\n",
    "### Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(count):\n",
    "    print('Trabajo %d lanzado! Resultado=%f'%(count,random.random()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = list()\n",
    "for i in range(3):\n",
    "    t = threading.Thread(target=worker, args=(i,))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for tt in threads:\n",
    "    tt.join()\n",
    "print(\"todos los trabajos terminaron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2\n",
    "Librerias como numpy ya contemplan en muchas de sus funciones optimizaciones y threads para acelerar el cómputo.\n",
    "En este caso veremos la función producto punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n=1000000\n",
    "a=np.random.rand(n)\n",
    "b=np.random.rand(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proceso serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "i=0\n",
    "pdot=0;\n",
    "while i<n:\n",
    "   pdot+=a[i]*b[i] \n",
    "   i=i+1\n",
    "print('Resultado =',pdot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### con numpy que debajo utiliza librerias de aceleracion como BLAS\n",
    "[https://es.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms](https://es.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pdot=np.dot(a,b)\n",
    "pdot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3 \n",
    "Threads para una funcion mas compleja, ordenar un vector random.\n",
    "\n",
    "En general ordenar un vector de n elementos requiere nlog(n) operaciones.\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Sorting_algorithm](https://en.wikipedia.org/wiki/Sorting_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from timeit import default_timer as timer\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contenido de functions.py\n",
    "\n",
    "`\n",
    "import numpy as np\n",
    "def createandsort (n):\n",
    " rand = np.random.RandomState(42) #semilla para reproducir mismos numeros\n",
    " a = rand.rand(n) #array de n elementos random\n",
    " return a.sort() #Ordena de menor a menor (nlogn nro de operaciones)\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 10000000\n",
    "#Creamos sizes con 3 arrays\n",
    "sizes = [vector_size for i in range(0,3)] #cada array es de tamaño vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usando funcion secuencial\n",
    "tic = timer()\n",
    "[functions.createandsort(size) for size in sizes]\n",
    "tac = timer()\n",
    "print(\"tiempo secuencial: \", tac-tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usando pool multiprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    pool = Pool(processes=3)\n",
    "    tic = timer()\n",
    "    pool.map(functions.createandsort,sizes)\n",
    "    tac = timer()\n",
    "    print(\"tiempo paralelo: \",tac-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4. \n",
    "Lectura y manejo de datos Pandas y Dask.\n",
    "\n",
    "Revisamos como leer un archivo de datos csv grande con pandas, y con dask que utiliza paralelizacion para acelerar funciones.\n",
    "\n",
    "[https://dask.org/](https://dask.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos un archivo dummy con 1 millon de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1000000\n",
    "val=np.random.rand(nrows,5)\n",
    "df = pd.DataFrame(data=val,columns=[\"col1\",\"col2\",\"col3\",\"col4\",\"col5\"])\n",
    "df.to_csv(\"dummy.csv\",header=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2=pd.read_csv(\"dummy.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura con Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df3=dd.read_csv(\"dummy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazo con pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res=df2['col1'].mask(df2['col1']>0.5,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazo con Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res=df3['col1'].mask(df3['col1']>0.5,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Memoria\n",
    "\n",
    "Para grandes volumenes de datos es importante el consumo de memoria RAM.\n",
    "\n",
    "Es importante manejar los tipos de datos minimos necesarios para representar la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = df2.memory_usage(index=False,deep=True).sum() / 1024 /1024\n",
    "print('Uso memoria %d MB'%mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas definiendo variables de 16bits para cada float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.read_csv(\"dummy.csv\",dtype={\"col1\":\"float16\",\"col2\":\"float16\",\"col3\":\"float16\",\"col4\":\"float16\",\"col5\":\"float16\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = df4.memory_usage(index=False,deep=True).sum() / 1024 /1024\n",
    "print('Uso memoria %d MB'%mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
